{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C2_P2_an√°lisis_sentimientos_Lafaurie.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOwbW9ziRHpOiP7YjsQaq+z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LauraGuio/MachineLearning/blob/master/C2_P2_an%C3%A1lisis_sentimientos_Lafaurie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkOPjZTII9L9"
      },
      "source": [
        "#An√°lisis de sentimientos con Twitter (DiaSinCarne)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaFwQ5j1KtZ9"
      },
      "source": [
        "**Cargar Librer√≠as**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2Iv_QjwKvnw"
      },
      "source": [
        "import os\n",
        "import tweepy as tw \n",
        "import pandas as pd"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbX3Z-xUK_2r"
      },
      "source": [
        "**Permisos de acceso desde Python a el api rest**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ8mqe9eLEfQ"
      },
      "source": [
        "consumer_key = 'W4tiErhtxNvpIWfT5B0wc4mTX'\n",
        "consumer_secret = 'ArG6Btx0j3zPj4YqGnZcA6QeOgYsixmZulWqz72nAioNzS1X8V'\n",
        "access_token = '1324841042726838273-8W7fCTYm7TpkrfL2rl6KfbdwW6eFXa'\n",
        "access_token_secret = 'Z26dV3Ji7wmBSylnxb3YXn1oRjpmgTHOQgsQVqNcBYbtf'\n",
        "\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tM_orOpNKnJ"
      },
      "source": [
        "**Consultando Tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNKGVjW8NMub",
        "outputId": "418094c3-b224-4ebb-f895-c1111147d70a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Definir el termino de la busqueda y la fecha de inicio\n",
        "search_words = '#DiaSinCarne'\n",
        "date_since = \"2019-01-01\"\n",
        "#no tomar retweets\n",
        "new_search = search_words+\" -filter:retweets\"\n",
        "new_search\n",
        "#Coleccionar tweets (2000 tweets)\n",
        "tweets = tw.Cursor(api.search, new_search, \"es\", date_since).items(2000)\n",
        "tweets"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tweepy.cursor.ItemIterator at 0x7f9633e19208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ozrBcMcOr6x"
      },
      "source": [
        "**Convertir Tweets en Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn7y0Q7sOvoL"
      },
      "source": [
        "data_frame = [[tweet.user.screen_name, tweet.user.location,tweet.text] for tweet in tweets]\n",
        "\n",
        "tw_dataframe = pd.DataFrame(data= data_frame , columns=[\"user\",\"location\",\"text\"])\n",
        "tw_dataframe\n",
        "#lo guardamos en un .csv\n",
        "tw_dataframe.to_csv('twitter_DiaSinCarne_data.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjVQPMrhPmyy"
      },
      "source": [
        "**Mostrar algunos datos de Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpuf9uQ2PqVE",
        "outputId": "61a7100b-19ba-4f34-f260-ba8dfc4e56ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "tw_dataframe.head(2000)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PedroLasprilla</td>\n",
              "      <td></td>\n",
              "      <td>@jaimeflozada @NoraTamayoR Catano aportante a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Psicotero</td>\n",
              "      <td></td>\n",
              "      <td>@lcvelez #DiaSinCarne as√≠ siempre comienza lo ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>jorge_milian</td>\n",
              "      <td>Barranquilla</td>\n",
              "      <td>Y que #DiaSinCarne por qu√© en su lugar no deja...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>George1Col</td>\n",
              "      <td></td>\n",
              "      <td>@micielin La narrativa consiste en un movimien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ANONYMOUSSS2019</td>\n",
              "      <td>#ElPetrismoSeCuraLeyendo</td>\n",
              "      <td>Ola de homicidios, atracos, violencia en Bogot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>GABRIELSOL88</td>\n",
              "      <td>Bogot√°, D.C., Colombia</td>\n",
              "      <td>#DiaSinCarne ac√° hacemos todo mal y al rev√©s !...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>DiegoCardenas95</td>\n",
              "      <td>Colombia</td>\n",
              "      <td>#DiaSinCarne \\n\\nEn Colombia varios hogares lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>PocoCaballero</td>\n",
              "      <td></td>\n",
              "      <td>#DiaSinCarne pero con carniceros. https://t.co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>UnaCervezaPues</td>\n",
              "      <td>In Spain</td>\n",
              "      <td>@jflafaurie ya dijo que eso del #DiaSinCarne c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>MarViToF</td>\n",
              "      <td>Bogot√°, D.C., üá®üá¥</td>\n",
              "      <td>Pero que hijuemadres ... hagamos el #DiaSinCar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows √ó 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 user  ...                                               text\n",
              "0      PedroLasprilla  ...  @jaimeflozada @NoraTamayoR Catano aportante a ...\n",
              "1           Psicotero  ...  @lcvelez #DiaSinCarne as√≠ siempre comienza lo ...\n",
              "2        jorge_milian  ...  Y que #DiaSinCarne por qu√© en su lugar no deja...\n",
              "3          George1Col  ...  @micielin La narrativa consiste en un movimien...\n",
              "4     ANONYMOUSSS2019  ...  Ola de homicidios, atracos, violencia en Bogot...\n",
              "...               ...  ...                                                ...\n",
              "1995     GABRIELSOL88  ...  #DiaSinCarne ac√° hacemos todo mal y al rev√©s !...\n",
              "1996  DiegoCardenas95  ...  #DiaSinCarne \\n\\nEn Colombia varios hogares lo...\n",
              "1997    PocoCaballero  ...  #DiaSinCarne pero con carniceros. https://t.co...\n",
              "1998   UnaCervezaPues  ...  @jflafaurie ya dijo que eso del #DiaSinCarne c...\n",
              "1999         MarViToF  ...  Pero que hijuemadres ... hagamos el #DiaSinCar...\n",
              "\n",
              "[2000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1dG1bWhwyHL"
      },
      "source": [
        "##1. Preprocesamiento de DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46Gb7OlsxIRY"
      },
      "source": [
        "**Cargar librer√≠as**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCs7Ib_hxRHT"
      },
      "source": [
        "import re                                #operaciones regulares para la b√∫squeda y manipulaci√≥n de cadenas\n",
        "from nltk import TweetTokenizer          #libreria para tokenizar\n",
        "from nltk.stem import SnowballStemmer    #algoritmo para clasificaci√≥n de palabras\n",
        "#variables para mejorar la escritura (opcional)\n",
        "NORMALIZE = 'normalize'\n",
        "REMOVE = 'remove'\n",
        "MENTION = 'twmention'\n",
        "HASHTAG = 'twhashtag'\n",
        "URL = 'twurl'\n",
        "LAUGH = 'twlaugh'\n",
        "\n",
        "#definir que el algoritmo de clasificaci√≥n use el idioma espa√±ol\n",
        "_stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "#definir una variable para la funcion de tokenizar (opcional)\n",
        "_tokenizer = TweetTokenizer().tokenize\n",
        "\n",
        "#variable para definir si quiero normalizar: normalize o eliminar: remove los hashtags, menciones y urls en los tweets\n",
        "_twitter_features=\"normalize\"\n",
        "#variable para definir si se desea tener convertir o no a la raiz de la palabra.\n",
        "_stemming=False"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ9-DEwPxXn0"
      },
      "source": [
        "**Quitar palabras coloquiales/ t√≠ldes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_k--cVUxcKH"
      },
      "source": [
        "#lista de conversi√≥n para quitar las tildes a las vocales.\n",
        "DIACRITICAL_VOWELS = [('√°','a'), ('√©','e'), ('√≠','i'), ('√≥','o'), ('√∫','u'), ('√º','u')]\n",
        "\n",
        "#lista para corregir algunas palabras coloquiales\n",
        "SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),\n",
        "         ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'tambi√©n'),\n",
        "         ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\\+','mas'),('ily','te quiero mucho')]"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noXU0KXKxug_"
      },
      "source": [
        "**Normalizar risas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvyUH5Xnxwfp"
      },
      "source": [
        "def normalize_laughs(message):\n",
        "  message = re.sub(r'\\b(?=\\w*[j])[aeioujs]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(?=\\w*[k])[aeiouks]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(juas+|lol)\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  return message"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF5hCBq1x9Gt"
      },
      "source": [
        "**Eliminar menciones, URL, #**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCxSegU7yAqW"
      },
      "source": [
        "def process_twitter_features(message, twitter_features):\n",
        "\n",
        "  message = re.sub(r'[\\.\\,]http','. http', message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'[\\.\\,]#', '. #', message)\n",
        "  message = re.sub(r'[\\.\\,]@', '. @', message)\n",
        "\n",
        "  if twitter_features == REMOVE:\n",
        "    # eliminar menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))(@|#)\\S+', '', message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', '', message, flags=re.IGNORECASE)\n",
        "  elif twitter_features == NORMALIZE:\n",
        "    # cuando sea necesario se normalizaran las menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))@\\S+', MENTION, message)\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))#\\S+', HASHTAG, message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', URL, message, flags=re.IGNORECASE)\n",
        "\n",
        "  return message"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoB9UfmZ3ESf"
      },
      "source": [
        "**Quitar emojis, caracteres especiales, etc**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLXNb5yS3Hhq"
      },
      "source": [
        "def preprocessor_emoji(text):\n",
        "  text = re.sub('<[^>]*>','', text)\n",
        "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
        "  text = (re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', ''))\n",
        "  return text"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPGQtf-uyHLh"
      },
      "source": [
        "**Preprocesamos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3m2D7xzyKBJ"
      },
      "source": [
        "def preprocess(message):\n",
        "  # convertir a minusculas\n",
        "  message = message.lower()\n",
        "        \n",
        "  # eliminar n√∫meros, retorno de linea y retweet\n",
        "  message = re.sub(r'(\\d+|\\n|\\brt\\b)', '', message)\n",
        "        \n",
        "  # elimar vocales con signos diacr√≠ticos\n",
        "  for s,t in DIACRITICAL_VOWELS:\n",
        "    message = re.sub(r'{0}'.format(s), t, message)\n",
        "        \n",
        "  # eliminar caracteres repetidos \n",
        "  message = re.sub(r'(.)\\1{2,}', r'\\1\\1', message)\n",
        "       \n",
        "  # normalizar las risas\n",
        "  message = normalize_laughs(message)\n",
        "        \n",
        "  # traducir la jerga y terminos coloquiales sobre todo en el espa√±ol\n",
        "  for s,t in SLANG:\n",
        "    message = re.sub(r'\\b{0}\\b'.format(s), t, message)\n",
        "\n",
        "  #normalizar/eliminar hashtags, menciones y URL\n",
        "  message = process_twitter_features(message, _twitter_features)\n",
        "\n",
        "  #eliminar emojis\n",
        "  message = preprocessor_emoji(message)\n",
        "\n",
        "  #Convertir las palabras a su raiz\n",
        "  if _stemming:\n",
        "    message = ' '.join(_stemmer.stem(w) for w in _tokenizer(message))\n",
        "\n",
        "  return message"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yhRQfByz7JI"
      },
      "source": [
        "##3. Aplicamos preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m01_ldjkz91j"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#aplicamos el preprocesamiento a los tweets con steaming =false\n",
        "tw_dataframe ['text'] = tw_dataframe['text'].apply(preprocess)\n",
        "#eliminamos la columna user y location\n",
        "#tw_dataframe = tw_dataframe.drop(columns=['user','location'], axis=1)\n",
        "#guardamos el dataset en un nuvevo CSV para facilitar su posterior uso\n",
        "tw_dataframe.to_csv('/content/dataset_Lafaurie_full_clean.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dypstw3wlCJU"
      },
      "source": [
        "##2. Preprocesamiento de corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvHmOuIRlIJQ"
      },
      "source": [
        "**Cargar librer√≠as**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE5I-Ap7lKA-"
      },
      "source": [
        "import re                                #operaciones regulares para la b√∫squeda y manipulaci√≥n de cadenas\n",
        "from nltk import TweetTokenizer          #libreria para tokenizar\n",
        "from nltk.stem import SnowballStemmer    #algoritmo para clasificaci√≥n de palabras\n",
        "#variables para mejorar la escritura (opcional)\n",
        "NORMALIZE = 'normalize'\n",
        "REMOVE = 'remove'\n",
        "MENTION = 'twmention'\n",
        "HASHTAG = 'twhashtag'\n",
        "URL = 'twurl'\n",
        "LAUGH = 'twlaugh'\n",
        "\n",
        "#definir que el algoritmo de clasificaci√≥n use el idioma espa√±ol\n",
        "_stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "#definir una variable para la funcion de tokenizar (opcional)\n",
        "_tokenizer = TweetTokenizer().tokenize\n",
        "\n",
        "#variable para definir si quiero normalizar: normalize o eliminar: remove los hashtags, menciones y urls en los tweets\n",
        "_twitter_features=\"normalize\"\n",
        "#variable para definir si se desea tener convertir o no a la raiz de la palabra.\n",
        "_stemming=False"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-edfwaRlOji"
      },
      "source": [
        "**Quitar palabras coloquiales y t√≠ldes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RR0ZI6HlTjA"
      },
      "source": [
        "#lista de conversi√≥n para quitar las tildes a las vocales.\n",
        "DIACRITICAL_VOWELS = [('√°','a'), ('√©','e'), ('√≠','i'), ('√≥','o'), ('√∫','u'), ('√º','u')]\n",
        "\n",
        "#lista para corregir algunas palabras coloquiales / jerga en espa√±ol (obviamente faltan m√°s)\n",
        "SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),\n",
        "         ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'tambi√©n'),\n",
        "         ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\\+','mas')]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1vKuXc5lXU-"
      },
      "source": [
        "**Normalizar risas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2L0_pDKlZT5"
      },
      "source": [
        "#metodo para normalizar las risas\n",
        "def normalize_laughs(message):\n",
        "  message = re.sub(r'\\b(?=\\w*[j])[aeiouj]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(?=\\w*[k])[aeiouk]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(juas+|lol)\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  return message"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnSm00KrlgMN"
      },
      "source": [
        "**Eliminar menciones, #, URL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA2JiLVGljSh"
      },
      "source": [
        "def process_twitter_features(message, twitter_features):\n",
        "\n",
        "  message = re.sub(r'[\\.\\,]http','. http', message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'[\\.\\,]#', '. #', message)\n",
        "  message = re.sub(r'[\\.\\,]@', '. @', message)\n",
        "\n",
        "  if twitter_features == REMOVE:\n",
        "    # eliminar menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))(@|#)\\S+', '', message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', '', message, flags=re.IGNORECASE)\n",
        "  elif twitter_features == NORMALIZE:\n",
        "    # cuando sea necesario se normalizaran las menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))@\\S+', MENTION, message)\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))#\\S+', HASHTAG, message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', URL, message, flags=re.IGNORECASE)\n",
        "\n",
        "  return message"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpi0-sTPlxIz"
      },
      "source": [
        "**Preprocesamiento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-ewxspZlzZy"
      },
      "source": [
        "def preprocess(message):\n",
        "  # convertir a minusculas\n",
        "  message = message.lower()\n",
        "        \n",
        "  # eliminar n√∫meros, retorno de linea y el tan odios retweet (de los viejos estilos de twitter)\n",
        "  message = re.sub(r'(\\d+|\\n|\\brt\\b)', '', message)\n",
        "        \n",
        "  # elimar vocales con signos diacr√≠ticos (posible ambig√ºedad)\n",
        "  for s,t in DIACRITICAL_VOWELS:\n",
        "    message = re.sub(r'{0}'.format(s), t, message)\n",
        "        \n",
        "  # eliminar caracteres repetidos \n",
        "  message = re.sub(r'(.)\\1{2,}', r'\\1\\1', message)\n",
        "       \n",
        "  # normalizar las risas\n",
        "  message = normalize_laughs(message)\n",
        "        \n",
        "  # traducir la jerga y terminos coloquiales sobre todo en el espa√±ol\n",
        "  for s,t in SLANG:\n",
        "    message = re.sub(r'\\b{0}\\b'.format(s), t, message)\n",
        "\n",
        "  #normalizar/eliminar hashtags, menciones y URL\n",
        "  message = process_twitter_features(message, _twitter_features)\n",
        "\n",
        "  #Convertir las palabras a su raiz ( Bonita, bonito) -> bonit \n",
        "  if _stemming:\n",
        "    message = ' '.join(_stemmer.stem(w) for w in _tokenizer(message))\n",
        "\n",
        "  return message"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dGrPsN9l6z5"
      },
      "source": [
        "**Descargar librer√≠a NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-l6pI98l-Zc",
        "outputId": "ccfba47d-af60-484f-d0a3-a2705230f66c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Descargamos la libreria de stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6CdPeVrmIFG"
      },
      "source": [
        "##3. Aplicar preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoPntHgRmOfS"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/luisFernandoCastellanosG/Machine_learning/master/An%C3%A1lisis%20de%20sentimientos%20en%20Twitter/espa%C3%B1ol/datasets/Corpus/dataset_2017_full.csv', encoding='utf-8')\n",
        "#asignamos nombres a las columnas del csv para facilitar la busqueda de informaci√≥n\n",
        "df.columns = ['tweetid', 'tweet','sentiment']\n",
        "#aplicamos el preprocesamiento a los tweets con steaming =false\n",
        "df['tweet'] = df['tweet'].apply(preprocess)\n",
        "#eliminamos la columna tweetid que no nos sirve para entrenar y si nos genera mas uso de memoria \n",
        "df = df.drop(columns=\"tweetid\")\n",
        "#Es mejor trabajar con valores enteros que con letras\n",
        "#por lo tanto reemplazaremos los sentimientos que estan como NONE->-1 | NEU -> 0 | P->1 | N->2\n",
        "df.loc[df['sentiment'] == 'NONE', 'sentiment'] = '-1'\n",
        "df.loc[df['sentiment'] == 'NEU', 'sentiment'] = '0'\n",
        "df.loc[df['sentiment'] == 'P', 'sentiment'] = '1'\n",
        "df.loc[df['sentiment'] == 'N', 'sentiment'] = '2'\n",
        "df[\"sentiment\"].unique()\n",
        "#guardamos el dataset en un nuvevo CSV para facilitar su posterior uso\n",
        "df.to_csv('/content/dataset_2017_full_clean.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSlo7O9smoH-"
      },
      "source": [
        "##4. Entrenar modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh6tCYJL7OgU"
      },
      "source": [
        "**Librer√≠as necesarias**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72WOER5V9HH7",
        "outputId": "d64e21f9-c6e0-407f-966e-f3caec0fc4e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sJ3eZyd7Qzz"
      },
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxEy-5p2mscO"
      },
      "source": [
        "**Tokenizar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMMeGgkvmurY",
        "outputId": "b74c5c1e-731b-492f-a1b9-e87f5eeb2507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#p2.1: funcion tokenizar con esteroides --tokeniza y limpia--\n",
        "print(\"p2.1: funcion tokenizar con esteroides --tokeniza y limpia--\")\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', '')\n",
        "    tokenized = [w for w in text.split() if w not in stop]\n",
        "    return tokenized\n",
        "#p2.2: funcion para extraer un documento del dataset  \n",
        "print(\"p2.2: funcion para extraer un documento del dataset  \")\n",
        "def stream_docs(path):\n",
        "    with open(path, 'r', encoding='utf-8') as csv:\n",
        "        next(csv)  # skip header\n",
        "        for line in csv:\n",
        "            text, label = line[:-3],  int(line[-2])\n",
        "            yield text, label\n",
        "#p2.3: funcion que tomara una secuencia de documentos y devolvera un n√∫mero particular de documentos\n",
        "def get_minibatch(doc_stream, size):\n",
        "    docs, y = [], []\n",
        "    try:\n",
        "        for _ in range(size):\n",
        "            text, label = next(doc_stream)\n",
        "            docs.append(text)\n",
        "            y.append(label)\n",
        "    except StopIteration:\n",
        "        return None, None\n",
        "    return docs, y"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p2.1: funcion tokenizar con esteroides --tokeniza y limpia--\n",
            "p2.2: funcion para extraer un documento del dataset  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIW3acfJm_L6"
      },
      "source": [
        "**Entrenamiento con modelo de regresi√≥n log√≠stica**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5rbuOKAnCZ7",
        "outputId": "6f7c7220-1ec4-4576-c3ba-3ba6507ac997",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "path='/content/dataset_2017_full_clean.csv'\n",
        "#p2: definimos una versi√≥n liviana de CountVectorizer+TfidfVectorizer llamada HashingVectorizer\n",
        "\n",
        "vect = HashingVectorizer(decode_error='ignore', \n",
        "                         n_features=2**21,\n",
        "                         preprocessor=None, \n",
        "                         tokenizer=tokenizer)\n",
        "\n",
        "#definimos como algoritmo la regressi√≥n logistica en el decenso gradiante \n",
        "\n",
        "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
        "doc_stream = stream_docs(path)\n",
        "#p3. entrenamos \n",
        "stop = stopwords.words('spanish')\n",
        "#pbar = pyprind.ProgBar(50)\n",
        "#definimos las clases con las cuales vamos a entrenar\n",
        "classes = np.array([-1,0, 1,2])\n",
        "#hacemos 50 repeticiones\n",
        "for _ in range(50):\n",
        "  #tomaremos grupos de 500 tweets para entrenar\n",
        "    X_train, y_train = get_minibatch(doc_stream, size=500)\n",
        "    if not X_train:\n",
        "        break\n",
        "    X_train = vect.transform(X_train)\n",
        "    clf.partial_fit(X_train, y_train, classes=classes)\n",
        "    #pbar.update()\n",
        "#probamos la eficiencia del modelo con 500 tweets .\n",
        "X_test, y_test = get_minibatch(doc_stream, size=500)\n",
        "X_test = vect.transform(X_test)\n",
        "print('Presici√≥n del modelo: %.3f' % clf.score(X_test, y_test))\n",
        "#recalibramos el modelo.\n",
        "clf = clf.partial_fit(X_test, y_test)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Presici√≥n del modelo: 0.844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LxhvIZEpUwF"
      },
      "source": [
        "##5. Recorremos el dataset de Tweets ya procesados y lo clasificamos dependiendo el Dataset TAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vafcDBSS-c24",
        "outputId": "00319a07-14a5-4f6a-d290-8632d0070406",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install pyprind"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyprind in /usr/local/lib/python3.6/dist-packages (2.11.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVf6n55V-VHA"
      },
      "source": [
        "import pyprind"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9weAEItspbxa",
        "outputId": "4c991729-a983-40f4-a34b-bf8be7a27aee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyprind\n",
        "\n",
        "pbar = pyprind.ProgBar(50000)\n",
        "\n",
        "df = pd.read_csv('/content/dataset_Lafaurie_full_clean.csv', encoding='utf-8')\n",
        "#creamos una columna llamada Sentimient donde guardaremos la predicci√≥n\n",
        "df['sentiment'] =''\n",
        "#creamos una columna llamada Probability donde guardaremos la acertabilidad que dio el clasificador\n",
        "df['probability']=0\n",
        "#conversi√≥n de sentimientos (numeros a palabras)= NONE->-1 | NEU -> 0 | P->1 | N->2\n",
        "label = {-1:'Sin sentimiento', 0:'Neutro', 1:'Positivo',2: 'Negativo'}\n",
        "for rowid in range(len(df.index)):\n",
        "  text=df['text'][rowid]\n",
        "  textConvert = vect.transform([text]) \n",
        "  df['sentiment'][rowid]=label[clf.predict(textConvert)[0]]\n",
        "  df['probability'][rowid]=np.max(clf.predict_proba(textConvert))*100\n",
        "  pbar.update()\n",
        "df.head(20)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "0% [#                             ] 100% | ETA: 00:34:52"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PedroLasprilla</td>\n",
              "      <td>NaN</td>\n",
              "      <td>twmention twmention catano aportante a campa√±a...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Psicotero</td>\n",
              "      <td>NaN</td>\n",
              "      <td>twmention twhashtag asi siempre comienza lo qu...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>jorge_milian</td>\n",
              "      <td>Barranquilla</td>\n",
              "      <td>y que twhashtag por que en su lugar no dejan d...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>George1Col</td>\n",
              "      <td>NaN</td>\n",
              "      <td>twmention la narrativa consiste en un movimien...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ANONYMOUSSS2019</td>\n",
              "      <td>#ElPetrismoSeCuraLeyendo</td>\n",
              "      <td>ola de homicidios atracos violencia en bogota ...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>JuanCar24388345</td>\n",
              "      <td>NaN</td>\n",
              "      <td>twmention a los de izquierda les gusta los deg...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>duvanzarama</td>\n",
              "      <td>NaN</td>\n",
              "      <td>twhashtag semana sin carne meses sin carne es ...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>elsalgadob</td>\n",
              "      <td>Bogot√°, Colombia.</td>\n",
              "      <td>el twhashtag de muchos comenzo con la pandemia</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>JuanJoDuarte1</td>\n",
              "      <td>Bucaramanga - Colombia</td>\n",
              "      <td>donde saquen el dia sin arroz nos joden a todo...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>FabianFontecha</td>\n",
              "      <td>Colombia</td>\n",
              "      <td>twmention algunos no estaban muy convencidos d...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>BogotaDiario</td>\n",
              "      <td>Bogot√°, D.C., Colombia</td>\n",
              "      <td>como entender que del twmention la corporacio...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>rnaturalmente</td>\n",
              "      <td>bogota colombia</td>\n",
              "      <td>twhashtag twhashtag esta sabrosura twurl</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>punk_pika</td>\n",
              "      <td>El infierno</td>\n",
              "      <td>twmention twmention twmention ya sabiendo que ...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>YennyRiver</td>\n",
              "      <td>NaN</td>\n",
              "      <td>por supuesto que a estas alturas del a√±o donde...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>YennyRiver</td>\n",
              "      <td>NaN</td>\n",
              "      <td>y si mejor la administracion pone su mirada en...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>pilarcoroneld</td>\n",
              "      <td>Colombia</td>\n",
              "      <td>bajo nivel de debate de twmention es una vergu...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>LaLectora14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>es una falta de respeto que claudia permita qu...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>VidalJaner1</td>\n",
              "      <td>Bogot√° Colombia</td>\n",
              "      <td>se imaginan que dejaramos de pensar en el twh...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Maferlizcano406</td>\n",
              "      <td>Colombia</td>\n",
              "      <td>mas del de los cereales que se cultivan en el ...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>feanciscobga</td>\n",
              "      <td>Bucaramanga, Colombia</td>\n",
              "      <td>twhashtag listo arrgg salud twurl</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               user                  location  ... sentiment probability\n",
              "0    PedroLasprilla                       NaN  ...  Positivo          87\n",
              "1         Psicotero                       NaN  ...  Positivo          94\n",
              "2      jorge_milian              Barranquilla  ...  Positivo          74\n",
              "3        George1Col                       NaN  ...  Positivo          82\n",
              "4   ANONYMOUSSS2019  #ElPetrismoSeCuraLeyendo  ...  Positivo          74\n",
              "5   JuanCar24388345                       NaN  ...  Positivo          84\n",
              "6       duvanzarama                       NaN  ...  Positivo          75\n",
              "7        elsalgadob         Bogot√°, Colombia.  ...  Positivo          81\n",
              "8     JuanJoDuarte1    Bucaramanga - Colombia  ...  Positivo          84\n",
              "9    FabianFontecha                  Colombia  ...  Positivo          86\n",
              "10     BogotaDiario    Bogot√°, D.C., Colombia  ...  Positivo          88\n",
              "11    rnaturalmente           bogota colombia  ...  Positivo          93\n",
              "12        punk_pika               El infierno  ...  Positivo          95\n",
              "13       YennyRiver                       NaN  ...  Positivo          85\n",
              "14       YennyRiver                       NaN  ...  Positivo          75\n",
              "15    pilarcoroneld                  Colombia  ...  Positivo          68\n",
              "16      LaLectora14                       NaN  ...  Positivo          76\n",
              "17      VidalJaner1           Bogot√° Colombia  ...  Positivo          82\n",
              "18  Maferlizcano406                  Colombia  ...  Positivo          81\n",
              "19     feanciscobga     Bucaramanga, Colombia  ...  Positivo          88\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrFfnXlFqb5f"
      },
      "source": [
        "def f_prediction(row):\n",
        "  text=row['text']\n",
        "  textConvert = vect.transform([text]) \n",
        "  return label[clf.predict(textConvert)[0]]\n",
        "\n",
        "def f_probability(row):\n",
        "  text=row['text']\n",
        "  textConvert = vect.transform([text]) \n",
        "  return np.max(clf.predict_proba(textConvert))*100\n",
        "\n",
        "df[\"sentiment\"] = df.apply(f_prediction, axis=1) # recorriendo columnas\n",
        "df[\"probability\"] = df.apply(f_probability, axis=1) # recorriendo columnas"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mxpqaPeqo4h",
        "outputId": "e500a5fc-ccee-41ca-c99f-c88a0fb032ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#sentimientos = df[\"sentiment\"].unique()\n",
        "df.groupby('sentiment')['location'].nunique().plot(kind='bar')\n",
        "print(df.groupby(['sentiment']).size())\n",
        "#df.groupby(['sentiment']).size().unstack().plot(kind='bar',stacked=True)\n",
        "plt.show()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentiment\n",
            "Negativo      18\n",
            "Positivo    1982\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}