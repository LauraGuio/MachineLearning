{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C2_P2_análisis_sentimientos_AMAS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMNGXbaKcfF5Drxnvn3ryzt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LauraGuio/MachineLearning/blob/master/C2_P2_an%C3%A1lisis_sentimientos_AMAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkOPjZTII9L9"
      },
      "source": [
        "#Análisis de sentimientos con Twitter (Premios Amas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaFwQ5j1KtZ9"
      },
      "source": [
        "**Cargar Librerías**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2Iv_QjwKvnw"
      },
      "source": [
        "import os\n",
        "import tweepy as tw \n",
        "import pandas as pd"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbX3Z-xUK_2r"
      },
      "source": [
        "**Permisos de acceso desde Python a el api rest**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ8mqe9eLEfQ"
      },
      "source": [
        "consumer_key = 'W4tiErhtxNvpIWfT5B0wc4mTX'\n",
        "consumer_secret = 'ArG6Btx0j3zPj4YqGnZcA6QeOgYsixmZulWqz72nAioNzS1X8V'\n",
        "access_token = '1324841042726838273-8W7fCTYm7TpkrfL2rl6KfbdwW6eFXa'\n",
        "access_token_secret = 'Z26dV3Ji7wmBSylnxb3YXn1oRjpmgTHOQgsQVqNcBYbtf'\n",
        "\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tM_orOpNKnJ"
      },
      "source": [
        "**Consultando Tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNKGVjW8NMub",
        "outputId": "e8dddf80-8b27-408d-ab04-a0303f0975c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Definir el termino de la busqueda y la fecha de inicio\n",
        "search_words = '#AMAs'\n",
        "date_since = \"2020-01-01\"\n",
        "#no tomar retweets\n",
        "new_search = search_words+\" -filter:retweets\"\n",
        "new_search\n",
        "#Coleccionar tweets (2000 tweets)\n",
        "tweets = tw.Cursor(api.search, new_search, \"es\", date_since).items(2000)\n",
        "tweets"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tweepy.cursor.ItemIterator at 0x7f7653c33908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ozrBcMcOr6x"
      },
      "source": [
        "**Convertir Tweets en Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn7y0Q7sOvoL",
        "outputId": "f76de292-8a0d-4f28-8aa7-057c4b86d728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "data_frame = [[tweet.user.screen_name, tweet.user.location,tweet.text] for tweet in tweets]\n",
        "\n",
        "tw_dataframe = pd.DataFrame(data= data_frame , columns=[\"user\",\"location\",\"text\"])\n",
        "tw_dataframe\n",
        "#lo guardamos en un .csv\n",
        "tw_dataframe.to_csv('twitter_AMAs_data.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-e110bce7603e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtw_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata_frame\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"location\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtw_dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#lo guardamos en un .csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-136-e110bce7603e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtw_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata_frame\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"location\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtw_dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#lo guardamos en un .csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;31m# Reached end of current page, get the next page...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__self__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# Set pagination mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m                                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_on_rate_limit_notify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                                         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rate limit reached. Sleeping for: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msleep_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sleep for few extra sec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;31m# if self.wait_on_rate_limit and self._reset_time is not None and \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjVQPMrhPmyy"
      },
      "source": [
        "**Mostrar algunos datos de Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpuf9uQ2PqVE",
        "outputId": "0bd6fbdc-3f45-472f-edde-59263ec0ce0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "tw_dataframe.head(2000)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>monieygs</td>\n",
              "      <td></td>\n",
              "      <td>Recién me conecto y veo que mis plegarias de t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vampsshadow</td>\n",
              "      <td>⋆ׅ࣪🍙݁⌯ㅤ</td>\n",
              "      <td>era fiesta familiar cumpleaños de la moza mejo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ffuchsia_17</td>\n",
              "      <td></td>\n",
              "      <td>no me quiero considerar fan todavía kakskskaka...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>justinxecuador</td>\n",
              "      <td></td>\n",
              "      <td>Aquí esperando a que voten \\nI’m voting for Ju...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Meleryn6</td>\n",
              "      <td></td>\n",
              "      <td>16. Te caen bien tus compañeros? \\nI am voting...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>TaeTaecitoo</td>\n",
              "      <td></td>\n",
              "      <td>PERO- LPM YOONGI CON MULLET.\\nMIN YOONGI ME SI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>Hazzastyles28_9</td>\n",
              "      <td>malabami</td>\n",
              "      <td>SIGAN VOTANDO QUE SI NO SE QUEJAN DESPUÉS\\n\\n1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>Kooromi_</td>\n",
              "      <td>💜P U R P L E 🇨🇷 W O R L D💜</td>\n",
              "      <td>Hermoso. Perfecta forma de empezar el día\\n¹⁰)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>Nightlighope</td>\n",
              "      <td>Mexico</td>\n",
              "      <td>Jimin dijo qué es el video más \"duro\" Miren qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>smokingress</td>\n",
              "      <td>lindemann</td>\n",
              "      <td>en fin pasó el enojó, sale desayunar\\n\\nI’m vo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 user  ...                                               text\n",
              "0            monieygs  ...  Recién me conecto y veo que mis plegarias de t...\n",
              "1         vampsshadow  ...  era fiesta familiar cumpleaños de la moza mejo...\n",
              "2         ffuchsia_17  ...  no me quiero considerar fan todavía kakskskaka...\n",
              "3      justinxecuador  ...  Aquí esperando a que voten \\nI’m voting for Ju...\n",
              "4            Meleryn6  ...  16. Te caen bien tus compañeros? \\nI am voting...\n",
              "...               ...  ...                                                ...\n",
              "1995      TaeTaecitoo  ...  PERO- LPM YOONGI CON MULLET.\\nMIN YOONGI ME SI...\n",
              "1996  Hazzastyles28_9  ...  SIGAN VOTANDO QUE SI NO SE QUEJAN DESPUÉS\\n\\n1...\n",
              "1997         Kooromi_  ...  Hermoso. Perfecta forma de empezar el día\\n¹⁰)...\n",
              "1998     Nightlighope  ...  Jimin dijo qué es el video más \"duro\" Miren qu...\n",
              "1999      smokingress  ...  en fin pasó el enojó, sale desayunar\\n\\nI’m vo...\n",
              "\n",
              "[2000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1dG1bWhwyHL"
      },
      "source": [
        "##1. Preprocesamiento de DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46Gb7OlsxIRY"
      },
      "source": [
        "**Cargar librerías**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCs7Ib_hxRHT"
      },
      "source": [
        "import re                                #operaciones regulares para la búsqueda y manipulación de cadenas\n",
        "from nltk import TweetTokenizer          #libreria para tokenizar\n",
        "from nltk.stem import SnowballStemmer    #algoritmo para clasificación de palabras\n",
        "#variables para mejorar la escritura (opcional)\n",
        "NORMALIZE = 'normalize'\n",
        "REMOVE = 'remove'\n",
        "MENTION = 'twmention'\n",
        "HASHTAG = 'twhashtag'\n",
        "URL = 'twurl'\n",
        "LAUGH = 'twlaugh'\n",
        "\n",
        "#definir que el algoritmo de clasificación use el idioma español\n",
        "_stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "#definir una variable para la funcion de tokenizar (opcional)\n",
        "_tokenizer = TweetTokenizer().tokenize\n",
        "\n",
        "#variable para definir si quiero normalizar: normalize o eliminar: remove los hashtags, menciones y urls en los tweets\n",
        "_twitter_features=\"normalize\"\n",
        "#variable para definir si se desea tener convertir o no a la raiz de la palabra.\n",
        "_stemming=False"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ9-DEwPxXn0"
      },
      "source": [
        "**Quitar palabras coloquiales/ tíldes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_k--cVUxcKH"
      },
      "source": [
        "#lista de conversión para quitar las tildes a las vocales.\n",
        "DIACRITICAL_VOWELS = [('á','a'), ('é','e'), ('í','i'), ('ó','o'), ('ú','u'), ('ü','u')]\n",
        "\n",
        "#lista para corregir algunas palabras coloquiales\n",
        "SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),\n",
        "         ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'también'),\n",
        "         ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\\+','mas'),('ily','te quiero mucho')]"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noXU0KXKxug_"
      },
      "source": [
        "**Normalizar risas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvyUH5Xnxwfp"
      },
      "source": [
        "def normalize_laughs(message):\n",
        "  message = re.sub(r'\\b(?=\\w*[j])[aeioujs]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(?=\\w*[k])[aeiouks]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(juas+|lol)\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  return message"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF5hCBq1x9Gt"
      },
      "source": [
        "**Eliminar menciones, URL, #**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCxSegU7yAqW"
      },
      "source": [
        "def process_twitter_features(message, twitter_features):\n",
        "\n",
        "  message = re.sub(r'[\\.\\,]http','. http', message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'[\\.\\,]#', '. #', message)\n",
        "  message = re.sub(r'[\\.\\,]@', '. @', message)\n",
        "\n",
        "  if twitter_features == REMOVE:\n",
        "    # eliminar menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))(@|#)\\S+', '', message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', '', message, flags=re.IGNORECASE)\n",
        "  elif twitter_features == NORMALIZE:\n",
        "    # cuando sea necesario se normalizaran las menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))@\\S+', MENTION, message)\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))#\\S+', HASHTAG, message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', URL, message, flags=re.IGNORECASE)\n",
        "\n",
        "  return message"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoB9UfmZ3ESf"
      },
      "source": [
        "**Quitar emojis, caracteres especiales, etc**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLXNb5yS3Hhq"
      },
      "source": [
        "def preprocessor_emoji(text):\n",
        "  text = re.sub('<[^>]*>','', text)\n",
        "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
        "  text = (re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', ''))\n",
        "  return text"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPGQtf-uyHLh"
      },
      "source": [
        "**Preprocesamos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3m2D7xzyKBJ"
      },
      "source": [
        "def preprocess(message):\n",
        "  # convertir a minusculas\n",
        "  message = message.lower()\n",
        "        \n",
        "  # eliminar números, retorno de linea y retweet\n",
        "  message = re.sub(r'(\\d+|\\n|\\brt\\b)', '', message)\n",
        "        \n",
        "  # elimar vocales con signos diacríticos\n",
        "  for s,t in DIACRITICAL_VOWELS:\n",
        "    message = re.sub(r'{0}'.format(s), t, message)\n",
        "        \n",
        "  # eliminar caracteres repetidos \n",
        "  message = re.sub(r'(.)\\1{2,}', r'\\1\\1', message)\n",
        "       \n",
        "  # normalizar las risas\n",
        "  message = normalize_laughs(message)\n",
        "        \n",
        "  # traducir la jerga y terminos coloquiales sobre todo en el español\n",
        "  for s,t in SLANG:\n",
        "    message = re.sub(r'\\b{0}\\b'.format(s), t, message)\n",
        "\n",
        "  #normalizar/eliminar hashtags, menciones y URL\n",
        "  message = process_twitter_features(message, _twitter_features)\n",
        "\n",
        "  #eliminar emojis\n",
        "  message = preprocessor_emoji(message)\n",
        "\n",
        "  #Convertir las palabras a su raiz\n",
        "  if _stemming:\n",
        "    message = ' '.join(_stemmer.stem(w) for w in _tokenizer(message))\n",
        "\n",
        "  return message"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yhRQfByz7JI"
      },
      "source": [
        "##3. Aplicamos preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m01_ldjkz91j"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#aplicamos el preprocesamiento a los tweets con steaming =false\n",
        "tw_dataframe ['text'] = tw_dataframe['text'].apply(preprocess)\n",
        "#eliminamos la columna user y location\n",
        "#tw_dataframe = tw_dataframe.drop(columns=['user','location'], axis=1)\n",
        "#guardamos el dataset en un nuvevo CSV para facilitar su posterior uso\n",
        "tw_dataframe.to_csv('/content/dataset_AMAs_full_clean.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dypstw3wlCJU"
      },
      "source": [
        "##2. Preprocesamiento de corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvHmOuIRlIJQ"
      },
      "source": [
        "**Cargar librerías**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE5I-Ap7lKA-"
      },
      "source": [
        "import re                                #operaciones regulares para la búsqueda y manipulación de cadenas\n",
        "from nltk import TweetTokenizer          #libreria para tokenizar\n",
        "from nltk.stem import SnowballStemmer    #algoritmo para clasificación de palabras\n",
        "#variables para mejorar la escritura (opcional)\n",
        "NORMALIZE = 'normalize'\n",
        "REMOVE = 'remove'\n",
        "MENTION = 'twmention'\n",
        "HASHTAG = 'twhashtag'\n",
        "URL = 'twurl'\n",
        "LAUGH = 'twlaugh'\n",
        "\n",
        "#definir que el algoritmo de clasificación use el idioma español\n",
        "_stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "#definir una variable para la funcion de tokenizar (opcional)\n",
        "_tokenizer = TweetTokenizer().tokenize\n",
        "\n",
        "#variable para definir si quiero normalizar: normalize o eliminar: remove los hashtags, menciones y urls en los tweets\n",
        "_twitter_features=\"normalize\"\n",
        "#variable para definir si se desea tener convertir o no a la raiz de la palabra.\n",
        "_stemming=False"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-edfwaRlOji"
      },
      "source": [
        "**Quitar palabras coloquiales y tíldes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RR0ZI6HlTjA"
      },
      "source": [
        "#lista de conversión para quitar las tildes a las vocales.\n",
        "DIACRITICAL_VOWELS = [('á','a'), ('é','e'), ('í','i'), ('ó','o'), ('ú','u'), ('ü','u')]\n",
        "\n",
        "#lista para corregir algunas palabras coloquiales / jerga en español (obviamente faltan más)\n",
        "SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),\n",
        "         ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'también'),\n",
        "         ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\\+','mas')]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1vKuXc5lXU-"
      },
      "source": [
        "**Normalizar risas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2L0_pDKlZT5"
      },
      "source": [
        "#metodo para normalizar las risas\n",
        "def normalize_laughs(message):\n",
        "  message = re.sub(r'\\b(?=\\w*[j])[aeiouj]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(?=\\w*[k])[aeiouk]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(juas+|lol)\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  return message"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnSm00KrlgMN"
      },
      "source": [
        "**Eliminar menciones, #, URL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA2JiLVGljSh"
      },
      "source": [
        "def process_twitter_features(message, twitter_features):\n",
        "\n",
        "  message = re.sub(r'[\\.\\,]http','. http', message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'[\\.\\,]#', '. #', message)\n",
        "  message = re.sub(r'[\\.\\,]@', '. @', message)\n",
        "\n",
        "  if twitter_features == REMOVE:\n",
        "    # eliminar menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))(@|#)\\S+', '', message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', '', message, flags=re.IGNORECASE)\n",
        "  elif twitter_features == NORMALIZE:\n",
        "    # cuando sea necesario se normalizaran las menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))@\\S+', MENTION, message)\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))#\\S+', HASHTAG, message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', URL, message, flags=re.IGNORECASE)\n",
        "\n",
        "  return message"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpi0-sTPlxIz"
      },
      "source": [
        "**Preprocesamiento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-ewxspZlzZy"
      },
      "source": [
        "def preprocess(message):\n",
        "  # convertir a minusculas\n",
        "  message = message.lower()\n",
        "        \n",
        "  # eliminar números, retorno de linea y el tan odios retweet (de los viejos estilos de twitter)\n",
        "  message = re.sub(r'(\\d+|\\n|\\brt\\b)', '', message)\n",
        "        \n",
        "  # elimar vocales con signos diacríticos (posible ambigüedad)\n",
        "  for s,t in DIACRITICAL_VOWELS:\n",
        "    message = re.sub(r'{0}'.format(s), t, message)\n",
        "        \n",
        "  # eliminar caracteres repetidos \n",
        "  message = re.sub(r'(.)\\1{2,}', r'\\1\\1', message)\n",
        "       \n",
        "  # normalizar las risas\n",
        "  message = normalize_laughs(message)\n",
        "        \n",
        "  # traducir la jerga y terminos coloquiales sobre todo en el español\n",
        "  for s,t in SLANG:\n",
        "    message = re.sub(r'\\b{0}\\b'.format(s), t, message)\n",
        "\n",
        "  #normalizar/eliminar hashtags, menciones y URL\n",
        "  message = process_twitter_features(message, _twitter_features)\n",
        "\n",
        "  #Convertir las palabras a su raiz ( Bonita, bonito) -> bonit \n",
        "  if _stemming:\n",
        "    message = ' '.join(_stemmer.stem(w) for w in _tokenizer(message))\n",
        "\n",
        "  return message"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dGrPsN9l6z5"
      },
      "source": [
        "**Descargar librería NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-l6pI98l-Zc",
        "outputId": "a374e9e7-092b-48c6-e31e-e6cf90d1c918",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Descargamos la libreria de stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6CdPeVrmIFG"
      },
      "source": [
        "##3. Aplicar preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoPntHgRmOfS"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/luisFernandoCastellanosG/Machine_learning/master/An%C3%A1lisis%20de%20sentimientos%20en%20Twitter/espa%C3%B1ol/datasets/Corpus/dataset_2017_full.csv', encoding='utf-8')\n",
        "#asignamos nombres a las columnas del csv para facilitar la busqueda de información\n",
        "df.columns = ['tweetid', 'tweet','sentiment']\n",
        "#aplicamos el preprocesamiento a los tweets con steaming =false\n",
        "df['tweet'] = df['tweet'].apply(preprocess)\n",
        "#eliminamos la columna tweetid que no nos sirve para entrenar y si nos genera mas uso de memoria \n",
        "df = df.drop(columns=\"tweetid\")\n",
        "#Es mejor trabajar con valores enteros que con letras\n",
        "#por lo tanto reemplazaremos los sentimientos que estan como NONE->-1 | NEU -> 0 | P->1 | N->2\n",
        "df.loc[df['sentiment'] == 'NONE', 'sentiment'] = '-1'\n",
        "df.loc[df['sentiment'] == 'NEU', 'sentiment'] = '0'\n",
        "df.loc[df['sentiment'] == 'P', 'sentiment'] = '1'\n",
        "df.loc[df['sentiment'] == 'N', 'sentiment'] = '2'\n",
        "df[\"sentiment\"].unique()\n",
        "#guardamos el dataset en un nuvevo CSV para facilitar su posterior uso\n",
        "df.to_csv('/content/dataset_2017_full_clean.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSlo7O9smoH-"
      },
      "source": [
        "##4. Entrenar modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh6tCYJL7OgU"
      },
      "source": [
        "**Librerías necesarias**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72WOER5V9HH7",
        "outputId": "6fd53d83-11fd-4555-bdd5-d70e8c0caab0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sJ3eZyd7Qzz"
      },
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxEy-5p2mscO"
      },
      "source": [
        "**Tokenizar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMMeGgkvmurY",
        "outputId": "95be9cc3-a8ab-40d6-c8e2-a67f1ed69f21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#p2.1: funcion tokenizar con esteroides --tokeniza y limpia--\n",
        "print(\"p2.1: funcion tokenizar con esteroides --tokeniza y limpia--\")\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', '')\n",
        "    tokenized = [w for w in text.split() if w not in stop]\n",
        "    return tokenized\n",
        "#p2.2: funcion para extraer un documento del dataset  \n",
        "print(\"p2.2: funcion para extraer un documento del dataset  \")\n",
        "def stream_docs(path):\n",
        "    with open(path, 'r', encoding='utf-8') as csv:\n",
        "        next(csv)  # skip header\n",
        "        for line in csv:\n",
        "            text, label = line[:-3],  int(line[-2])\n",
        "            yield text, label\n",
        "#p2.3: funcion que tomara una secuencia de documentos y devolvera un número particular de documentos\n",
        "def get_minibatch(doc_stream, size):\n",
        "    docs, y = [], []\n",
        "    try:\n",
        "        for _ in range(size):\n",
        "            text, label = next(doc_stream)\n",
        "            docs.append(text)\n",
        "            y.append(label)\n",
        "    except StopIteration:\n",
        "        return None, None\n",
        "    return docs, y"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p2.1: funcion tokenizar con esteroides --tokeniza y limpia--\n",
            "p2.2: funcion para extraer un documento del dataset  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIW3acfJm_L6"
      },
      "source": [
        "**Entrenamiento con modelo de regresión logística**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5rbuOKAnCZ7",
        "outputId": "f227d0b6-50d4-4a89-b93b-d25707c1142c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "path='/content/dataset_2017_full_clean.csv'\n",
        "#p2: definimos una versión liviana de CountVectorizer+TfidfVectorizer llamada HashingVectorizer\n",
        "\n",
        "vect = HashingVectorizer(decode_error='ignore', \n",
        "                         n_features=2**21,\n",
        "                         preprocessor=None, \n",
        "                         tokenizer=tokenizer)\n",
        "\n",
        "#definimos como algoritmo la regressión logistica en el decenso gradiante \n",
        "\n",
        "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
        "doc_stream = stream_docs(path)\n",
        "#p3. entrenamos \n",
        "stop = stopwords.words('spanish')\n",
        "#pbar = pyprind.ProgBar(50)\n",
        "#definimos las clases con las cuales vamos a entrenar\n",
        "classes = np.array([-1,0, 1,2])\n",
        "#hacemos 50 repeticiones\n",
        "for _ in range(50):\n",
        "  #tomaremos grupos de 500 tweets para entrenar\n",
        "    X_train, y_train = get_minibatch(doc_stream, size=500)\n",
        "    if not X_train:\n",
        "        break\n",
        "    X_train = vect.transform(X_train)\n",
        "    clf.partial_fit(X_train, y_train, classes=classes)\n",
        "    #pbar.update()\n",
        "#probamos la eficiencia del modelo con 500 tweets .\n",
        "X_test, y_test = get_minibatch(doc_stream, size=500)\n",
        "X_test = vect.transform(X_test)\n",
        "print('Presición del modelo: %.3f' % clf.score(X_test, y_test))\n",
        "#recalibramos el modelo.\n",
        "clf = clf.partial_fit(X_test, y_test)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Presición del modelo: 0.844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LxhvIZEpUwF"
      },
      "source": [
        "##5. Recorremos el dataset de Tweets ya procesados y lo clasificamos dependiendo el Dataset TAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vafcDBSS-c24",
        "outputId": "5770f3a6-53a6-45ca-cd07-5119a8479082",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install pyprind"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyprind in /usr/local/lib/python3.6/dist-packages (2.11.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVf6n55V-VHA"
      },
      "source": [
        "import pyprind"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9weAEItspbxa",
        "outputId": "981076a2-e3da-4a81-e1db-9dbbc7d6cf5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyprind\n",
        "\n",
        "pbar = pyprind.ProgBar(50000)\n",
        "\n",
        "df = pd.read_csv('/content/dataset_AMAs_full_clean.csv', encoding='utf-8')\n",
        "#creamos una columna llamada Sentimient donde guardaremos la predicción\n",
        "df['sentiment'] =''\n",
        "#creamos una columna llamada Probability donde guardaremos la acertabilidad que dio el clasificador\n",
        "df['probability']=0\n",
        "#conversión de sentimientos (numeros a palabras)= NONE->-1 | NEU -> 0 | P->1 | N->2\n",
        "label = {-1:'Sin sentimiento', 0:'Neutro', 1:'Positivo',2: 'Negativo'}\n",
        "for rowid in range(len(df.index)):\n",
        "  text=df['text'][rowid]\n",
        "  textConvert = vect.transform([text]) \n",
        "  df['sentiment'][rowid]=label[clf.predict(textConvert)[0]]\n",
        "  df['probability'][rowid]=np.max(clf.predict_proba(textConvert))*100\n",
        "  pbar.update()\n",
        "df.head(20)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "0% [#                             ] 100% | ETA: 00:34:39"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>monieygs</td>\n",
              "      <td>NaN</td>\n",
              "      <td>recien me conecto y veo que mis plegarias de t...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vampsshadow</td>\n",
              "      <td>⋆ׅ࣪🍙݁⌯ㅤ</td>\n",
              "      <td>era fiesta familiar cumpleaños de la moza mejo...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ffuchsia_17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no me quiero considerar fan todavia twlaugh pe...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>justinxecuador</td>\n",
              "      <td>NaN</td>\n",
              "      <td>aqui esperando a que voten i m voting for just...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Meleryn6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>te caen bien tus compañeros i am voting for f...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ChioLotr</td>\n",
              "      <td>NaN</td>\n",
              "      <td>twmention twmention yo lista para salir espera...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>xxminimonixx3</td>\n",
              "      <td>🖤: BTS &amp; TXT</td>\n",
              "      <td>empiezo con las cosas estas mucho bardo ya twl...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>vminchair</td>\n",
              "      <td>어쩜 이 밤의 표정이 이토록 또 아름다운 건 ☀\\nOh 저 별들도 불빛도 아닌 우리...</td>\n",
              "      <td>recordatorio recuerden dejar sus votos diario...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>joonminsoft</td>\n",
              "      <td>only army</td>\n",
              "      <td>twmention twmention si estoy escuchando friend...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>_Littlecrab</td>\n",
              "      <td>OnlyARMY🇲🇽</td>\n",
              "      <td>mi aportacion sera cuentas de yt y de sp aunqu...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>LoaInLuv</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no se donde lo lei pero los trenes podrian sig...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ivo_Oh21</td>\n",
              "      <td>Honduras</td>\n",
              "      <td>twmention me robe la foto i m voting for exo f...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>jungk_09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>twmention twmention pues aca dejo mi voto aai ...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>sweet_habitx</td>\n",
              "      <td>neverland</td>\n",
              "      <td>uy un poco de gente muy larga la lista i m vot...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>patyJMBTS</td>\n",
              "      <td>Distrito Federal, México</td>\n",
              "      <td>ando ocupando amix en chaoaedol que devuelvan ...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>cafujk_</td>\n",
              "      <td>NaN</td>\n",
              "      <td>twmention tengo mots persona y pronto mots per...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>NaliYoongi</td>\n",
              "      <td>Argentina🇦🇷</td>\n",
              "      <td>no se si o hacer un gc con mis moots de confia...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>dolcenini_</td>\n",
              "      <td>ʽ愛</td>\n",
              "      <td>espero que chanyeol este bieni m voting for ex...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>levelofcoward</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ya aprendi a dar rts fuck you actualizacion i ...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>spookyjimwalls</td>\n",
              "      <td>heartbreak weather</td>\n",
              "      <td>una vez que lo vez no podes dejar de verloi m ...</td>\n",
              "      <td>Positivo</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              user  ... probability\n",
              "0         monieygs  ...          81\n",
              "1      vampsshadow  ...          82\n",
              "2      ffuchsia_17  ...          86\n",
              "3   justinxecuador  ...          86\n",
              "4         Meleryn6  ...          86\n",
              "5         ChioLotr  ...          84\n",
              "6    xxminimonixx3  ...          86\n",
              "7        vminchair  ...          85\n",
              "8      joonminsoft  ...          89\n",
              "9      _Littlecrab  ...          64\n",
              "10        LoaInLuv  ...          49\n",
              "11        ivo_Oh21  ...          89\n",
              "12        jungk_09  ...          90\n",
              "13    sweet_habitx  ...          82\n",
              "14       patyJMBTS  ...          82\n",
              "15         cafujk_  ...          74\n",
              "16      NaliYoongi  ...          85\n",
              "17      dolcenini_  ...          83\n",
              "18   levelofcoward  ...          75\n",
              "19  spookyjimwalls  ...          74\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrFfnXlFqb5f"
      },
      "source": [
        "def f_prediction(row):\n",
        "  text=row['text']\n",
        "  textConvert = vect.transform([text]) \n",
        "  return label[clf.predict(textConvert)[0]]\n",
        "\n",
        "def f_probability(row):\n",
        "  text=row['text']\n",
        "  textConvert = vect.transform([text]) \n",
        "  return np.max(clf.predict_proba(textConvert))*100\n",
        "\n",
        "df[\"sentiment\"] = df.apply(f_prediction, axis=1) # recorriendo columnas\n",
        "df[\"probability\"] = df.apply(f_probability, axis=1) # recorriendo columnas"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mxpqaPeqo4h",
        "outputId": "4a228a9a-a5f9-456a-eaaa-9b1f27b03697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#sentimientos = df[\"sentiment\"].unique()\n",
        "df.groupby('sentiment')['location'].nunique().plot(kind='bar')\n",
        "print(df.groupby(['sentiment']).size())\n",
        "#df.groupby(['sentiment']).size().unstack().plot(kind='bar',stacked=True)\n",
        "plt.show()"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentiment\n",
            "Negativo       6\n",
            "Positivo    1994\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}